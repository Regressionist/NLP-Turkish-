{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self,path='data/HPBooks/resultant.txt',batch_size=64, val_size=10):\n",
    "        self.path=path\n",
    "        self.batch_size = batch_size\n",
    "        self.val_size = val_size\n",
    "        self.total_batch_size = batch_size+val_size\n",
    "        self.minibatch_index = 0\n",
    "        \n",
    "        self.prep_data()\n",
    "        \n",
    "    def prep_data(self):\n",
    "        subword_counter = Counter()\n",
    "        num_of_tokens = 0\n",
    "        with open(self.path,'r') as f:\n",
    "            for line in f:\n",
    "                line_subword = re.findall(r\"\\w+|[^\\w\\s]\", line[:-1].lower(), re.UNICODE)+['\\n']\n",
    "                subword_counter.update(line_subword)\n",
    "                num_of_tokens += len(line_subword)\n",
    "\n",
    "        self.subwords_itos = ['_unk_','_pad_','_eos_','_bos_'] + sorted(subword_counter,key=subword_counter.get,reverse=True)\n",
    "        self.subwords_stoi = defaultdict(lambda:0,{k:i for i,k in enumerate(self.subwords_itos)})\n",
    "\n",
    "        ids = torch.LongTensor(num_of_tokens)\n",
    "        token = 0\n",
    "        with open(self.path,'r') as f:\n",
    "            for line in f:\n",
    "                line_subword = re.findall(r\"\\w+|[^\\w\\s]\", line[:-1].lower(), re.UNICODE)+['\\n'] \n",
    "                np_arr = np.array([self.subwords_stoi[s] for s in line_subword],np.int32)\n",
    "                \n",
    "                try:\n",
    "                    ids[token:token+len(line_subword)] = torch.from_numpy(np_arr)\n",
    "                except:\n",
    "                    print(np_arr.shape, ids[token:token+len(line_subword)].shape)\n",
    "                    print(line)\n",
    "                token += len(line_subword)\n",
    "\n",
    "        num_batches = ids.size(0) // (self.total_batch_size)\n",
    "        ids = ids[:num_batches*self.total_batch_size]\n",
    "        self.full_data = ids.view(self.total_batch_size, -1)\n",
    "        \n",
    "    def get_minibatch(self,bptt=120):\n",
    "        if (self.minibatch_index + bptt+1 > self.full_data.size(1)):\n",
    "            self.minibatch_index=0\n",
    "        self.last_mbatch_x = self.full_data[:self.batch_size,self.minibatch_index:self.minibatch_index+bptt]\n",
    "        self.last_mbatch_y = self.full_data[:self.batch_size,1+self.minibatch_index:1+self.minibatch_index+bptt]\n",
    "        self.last_mbatch_x_val = self.full_data[self.batch_size:,self.minibatch_index:self.minibatch_index+bptt]\n",
    "        self.last_mbatch_y_val = self.full_data[self.batch_size:,1+self.minibatch_index:1+self.minibatch_index+bptt]\n",
    "        self.minibatch_index+=bptt\n",
    "        return(self.last_mbatch_x,self.last_mbatch_y,self.last_mbatch_x_val,self.last_mbatch_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14839"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(corpus.subwords_itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, dp_prob):\n",
    "        super(LangModel,self).__init__()\n",
    "    \n",
    "        self.input_size=input_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.dropout_prob=dp_prob\n",
    "    \n",
    "        self.dropout = nn.Dropout(dp_prob)\n",
    "        self.emb_layer=nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn=nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
    "        self.Linear=nn.Linear(2*hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, input_sentence, init_hidden_state):\n",
    "        #input_sentence: seq_len*batch_size\n",
    "    \n",
    "        emb=self.dropout(self.emb_layer(input_sentence))\n",
    "        #emb: seq_len*batch_size*emb_size\n",
    "    \n",
    "        output, hidden=self.rnn(emb, init_hidden_state)\n",
    "        #hidden: num_layers * num_directions, batch, hidden_size\n",
    "    \n",
    "        output=self.Linear(output.view(-1,2*hidden_size))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch(inputs, targets, hidden_size, mini_batch_size, model, model_optimizer, criterion, device=device):\n",
    "    model_optimizer.zero_grad()\n",
    "    hidden_state=(torch.zeros(2, mini_batch_size, hidden_size, device=device)).detach()\n",
    "    outputs = model(inputs, hidden_state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(inputs, targets, hidden_size, validation_size,criterion, device=device):\n",
    "    with torch.no_grad():\n",
    "        hidden_state=(torch.zeros(2, validation_size, hidden_size, device=device)).detach()\n",
    "        outputs = model(inputs, hidden_state)\n",
    "        val_loss = criterion(outputs, targets.reshape(-1))\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LangModel(input_size=vocab_size, embedding_size=300, hidden_size=512, dp_prob=0.2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "model_optimizer=optim.Adam(model.parameters())\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=512\n",
    "mini_batch_size=64\n",
    "val_size=10\n",
    "tl=[]\n",
    "vl=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1.0/100 | Training Loss: 337.9098275899887 | Validation Loss: 362.18280029296875\n",
      "%---Saving the model---%\n",
      "Step: 2.0/100 | Training Loss: 103.01831305027008 | Validation Loss: 132.1798095703125\n",
      "%---Saving the model---%\n",
      "Step: 3.0/100 | Training Loss: 37.729195564985275 | Validation Loss: 71.0864486694336\n",
      "%---Saving the model---%\n",
      "Step: 4.0/100 | Training Loss: 15.374464884400368 | Validation Loss: 54.47996139526367\n",
      "%---Saving the model---%\n",
      "Step: 5.0/100 | Training Loss: 7.290750823915005 | Validation Loss: 46.559383392333984\n",
      "%---Saving the model---%\n",
      "Step: 6.0/100 | Training Loss: 4.80288577824831 | Validation Loss: 45.715476989746094\n",
      "%---Saving the model---%\n",
      "Step: 7.0/100 | Training Loss: 3.819447632879019 | Validation Loss: 44.37186813354492\n",
      "%---Saving the model---%\n",
      "Step: 8.0/100 | Training Loss: 3.201359013095498 | Validation Loss: 43.390625\n",
      "%---Saving the model---%\n",
      "Step: 9.0/100 | Training Loss: 2.7470722012221813 | Validation Loss: 41.88705825805664\n",
      "%---Saving the model---%\n",
      "Step: 10.0/100 | Training Loss: 2.388103311881423 | Validation Loss: 42.22190856933594\n",
      "Step: 11.0/100 | Training Loss: 2.1003274098038673 | Validation Loss: 42.35795211791992\n",
      "Step: 12.0/100 | Training Loss: 1.8740285774692893 | Validation Loss: 40.805057525634766\n",
      "%---Saving the model---%\n",
      "Step: 13.0/100 | Training Loss: 1.6422621086239815 | Validation Loss: 40.775367736816406\n",
      "%---Saving the model---%\n",
      "Step: 14.0/100 | Training Loss: 1.447280501946807 | Validation Loss: 40.48500061035156\n",
      "%---Saving the model---%\n",
      "Step: 15.0/100 | Training Loss: 1.2809620024636388 | Validation Loss: 41.159637451171875\n",
      "Step: 16.0/100 | Training Loss: 1.1357680312357843 | Validation Loss: 39.09086608886719\n",
      "%---Saving the model---%\n",
      "Step: 17.0/100 | Training Loss: 0.9953881222754717 | Validation Loss: 39.99172592163086\n",
      "Step: 18.0/100 | Training Loss: 0.8614870584569871 | Validation Loss: 39.15653610229492\n",
      "Step: 19.0/100 | Training Loss: 0.759722804185003 | Validation Loss: 40.32485580444336\n",
      "Step: 20.0/100 | Training Loss: 0.659730841871351 | Validation Loss: 37.836448669433594\n",
      "%---Saving the model---%\n",
      "Step: 21.0/100 | Training Loss: 0.5621709628030658 | Validation Loss: 39.18378448486328\n",
      "Step: 22.0/100 | Training Loss: 0.48469157982617617 | Validation Loss: 38.85737991333008\n",
      "Step: 23.0/100 | Training Loss: 0.41681920853443444 | Validation Loss: 38.68850326538086\n",
      "Step: 24.0/100 | Training Loss: 0.3586447590496391 | Validation Loss: 37.61543273925781\n",
      "%---Saving the model---%\n",
      "Step: 25.0/100 | Training Loss: 0.3042090234812349 | Validation Loss: 38.215511322021484\n",
      "Step: 26.0/100 | Training Loss: 0.25806718843523413 | Validation Loss: 38.802879333496094\n",
      "Step: 27.0/100 | Training Loss: 0.22177001321688294 | Validation Loss: 37.54917907714844\n",
      "%---Saving the model---%\n",
      "Step: 28.0/100 | Training Loss: 0.19067646958865225 | Validation Loss: 37.446224212646484\n",
      "%---Saving the model---%\n",
      "Step: 29.0/100 | Training Loss: 0.1642687537241727 | Validation Loss: 37.56255340576172\n",
      "Step: 30.0/100 | Training Loss: 0.14192418346647173 | Validation Loss: 38.50117111206055\n",
      "Step: 31.0/100 | Training Loss: 0.12554075644584373 | Validation Loss: 36.81623077392578\n",
      "%---Saving the model---%\n",
      "Step: 32.0/100 | Training Loss: 0.1105219399323687 | Validation Loss: 37.4495964050293\n",
      "Step: 33.0/100 | Training Loss: 0.09703112818533555 | Validation Loss: 36.84535598754883\n",
      "Step: 34.0/100 | Training Loss: 0.08729868190130219 | Validation Loss: 38.25293731689453\n",
      "Step: 35.0/100 | Training Loss: 0.07756027742289007 | Validation Loss: 36.00959396362305\n",
      "%---Saving the model---%\n",
      "Step: 36.0/100 | Training Loss: 0.06865554803516716 | Validation Loss: 37.38543701171875\n",
      "Step: 37.0/100 | Training Loss: 0.06150337588042021 | Validation Loss: 37.20893096923828\n",
      "Step: 38.0/100 | Training Loss: 0.05615361322998069 | Validation Loss: 37.06303787231445\n",
      "Step: 39.0/100 | Training Loss: 0.050338542758254334 | Validation Loss: 36.16712951660156\n",
      "Step: 40.0/100 | Training Loss: 0.04639719519764185 | Validation Loss: 36.81759262084961\n",
      "Step: 41.0/100 | Training Loss: 0.041709176963195205 | Validation Loss: 37.298980712890625\n",
      "Step: 42.0/100 | Training Loss: 0.0390268768533133 | Validation Loss: 36.45943832397461\n",
      "Step: 43.0/100 | Training Loss: 0.035917624860303476 | Validation Loss: 36.13300323486328\n",
      "Step: 44.0/100 | Training Loss: 0.032754776591900736 | Validation Loss: 36.594085693359375\n",
      "Step: 45.0/100 | Training Loss: 0.030737588269403204 | Validation Loss: 37.22761917114258\n",
      "Step: 46.0/100 | Training Loss: 0.02870009701291565 | Validation Loss: 35.96567916870117\n",
      "%---Saving the model---%\n",
      "Step: 47.0/100 | Training Loss: 0.026698325789766386 | Validation Loss: 36.55908966064453\n",
      "Step: 48.0/100 | Training Loss: 0.02477572890347801 | Validation Loss: 36.05511474609375\n",
      "Step: 49.0/100 | Training Loss: 0.023627959468285553 | Validation Loss: 37.405025482177734\n",
      "Step: 50.0/100 | Training Loss: 0.02185426627693232 | Validation Loss: 35.47165298461914\n",
      "%---Saving the model---%\n",
      "Step: 51.0/100 | Training Loss: 0.020276882525649853 | Validation Loss: 36.50175857543945\n",
      "Step: 52.0/100 | Training Loss: 0.01882740516157355 | Validation Loss: 36.30955123901367\n",
      "Step: 53.0/100 | Training Loss: 0.01785104647569824 | Validation Loss: 36.74818420410156\n",
      "Step: 54.0/100 | Training Loss: 0.016881389674381353 | Validation Loss: 35.51023864746094\n",
      "Step: 55.0/100 | Training Loss: 0.015761842063511722 | Validation Loss: 36.393558502197266\n",
      "Step: 56.0/100 | Training Loss: 0.01484811406407971 | Validation Loss: 36.7446403503418\n",
      "Step: 57.0/100 | Training Loss: 0.013721182986046188 | Validation Loss: 35.96107482910156\n",
      "Step: 58.0/100 | Training Loss: 0.013148141144483816 | Validation Loss: 35.681556701660156\n",
      "Step: 59.0/100 | Training Loss: 0.012519284064183012 | Validation Loss: 36.25074768066406\n",
      "Step: 60.0/100 | Training Loss: 0.011529975265148096 | Validation Loss: 36.59453582763672\n",
      "Step: 61.0/100 | Training Loss: 0.010966451103740837 | Validation Loss: 35.74094009399414\n",
      "Step: 62.0/100 | Training Loss: 0.01024720258283196 | Validation Loss: 36.045623779296875\n",
      "Step: 63.0/100 | Training Loss: 0.009727566939545795 | Validation Loss: 35.649837493896484\n",
      "Step: 64.0/100 | Training Loss: 0.009938494891684968 | Validation Loss: 36.86841583251953\n",
      "Step: 65.0/100 | Training Loss: 0.009274573632865213 | Validation Loss: 34.95345687866211\n",
      "%---Saving the model---%\n",
      "Step: 66.0/100 | Training Loss: 0.008938346079958137 | Validation Loss: 36.366336822509766\n",
      "Step: 67.0/100 | Training Loss: 0.010301791378878988 | Validation Loss: 35.81882095336914\n",
      "Step: 68.0/100 | Training Loss: 0.009926342456310522 | Validation Loss: 36.56022262573242\n",
      "Step: 69.0/100 | Training Loss: 0.009046865750860889 | Validation Loss: 35.194210052490234\n",
      "Step: 70.0/100 | Training Loss: 0.008747052394028287 | Validation Loss: 36.328712463378906\n",
      "Step: 71.0/100 | Training Loss: 0.007682077291974565 | Validation Loss: 36.23486328125\n",
      "Step: 72.0/100 | Training Loss: 0.006704527273541316 | Validation Loss: 35.90134048461914\n",
      "Step: 73.0/100 | Training Loss: 0.007250844639202114 | Validation Loss: 35.32676696777344\n",
      "Step: 74.0/100 | Training Loss: 0.00692236980466987 | Validation Loss: 36.06586456298828\n",
      "Step: 75.0/100 | Training Loss: 0.007025202139629982 | Validation Loss: 36.6434211730957\n",
      "Step: 76.0/100 | Training Loss: 0.005843224735144759 | Validation Loss: 35.45661926269531\n",
      "Step: 77.0/100 | Training Loss: 0.005170171065401519 | Validation Loss: 35.90094757080078\n",
      "Step: 78.0/100 | Training Loss: 0.004627665635780431 | Validation Loss: 35.654815673828125\n",
      "Step: 79.0/100 | Training Loss: 0.004283724480046658 | Validation Loss: 36.69295883178711\n",
      "Step: 80.0/100 | Training Loss: 0.0040514177235309035 | Validation Loss: 34.950626373291016\n",
      "%---Saving the model---%\n",
      "Step: 81.0/100 | Training Loss: 0.004027974220662145 | Validation Loss: 36.05678939819336\n",
      "Step: 82.0/100 | Training Loss: 0.0036555736060108757 | Validation Loss: 35.60731887817383\n",
      "Step: 83.0/100 | Training Loss: 0.0034071045574819436 | Validation Loss: 36.69991683959961\n",
      "Step: 84.0/100 | Training Loss: 0.003151476515995455 | Validation Loss: 34.887542724609375\n",
      "%---Saving the model---%\n",
      "Step: 85.0/100 | Training Loss: 0.0030314245686895447 | Validation Loss: 36.0911979675293\n",
      "Step: 86.0/100 | Training Loss: 0.002888516219172743 | Validation Loss: 35.98483657836914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 87.0/100 | Training Loss: 0.0026814182620000793 | Validation Loss: 35.79955291748047\n",
      "Step: 88.0/100 | Training Loss: 0.003089882311542169 | Validation Loss: 35.13523864746094\n",
      "Step: 89.0/100 | Training Loss: 0.13051036297838436 | Validation Loss: 36.702144622802734\n",
      "Step: 90.0/100 | Training Loss: 0.7139925598166883 | Validation Loss: 40.51657485961914\n",
      "Step: 91.0/100 | Training Loss: 0.23780387663282454 | Validation Loss: 38.261783599853516\n",
      "Step: 92.0/100 | Training Loss: 0.07293860662321094 | Validation Loss: 38.20721435546875\n",
      "Step: 93.0/100 | Training Loss: 0.02848907030420378 | Validation Loss: 38.04297637939453\n",
      "Step: 94.0/100 | Training Loss: 0.015421571748447604 | Validation Loss: 38.574913024902344\n",
      "Step: 95.0/100 | Training Loss: 0.010865671189094428 | Validation Loss: 36.71501922607422\n",
      "Step: 96.0/100 | Training Loss: 0.009134237188845873 | Validation Loss: 37.85733413696289\n",
      "Step: 97.0/100 | Training Loss: 0.006865419178211596 | Validation Loss: 37.049808502197266\n",
      "Step: 98.0/100 | Training Loss: 0.007127973411115818 | Validation Loss: 38.452171325683594\n",
      "Step: 99.0/100 | Training Loss: 0.006419734436349245 | Validation Loss: 36.27268981933594\n",
      "Step: 100.0/100 | Training Loss: 0.004797875866643153 | Validation Loss: 37.71099853515625\n"
     ]
    }
   ],
   "source": [
    "batch_training_loss=0\n",
    "batch_validation_loss=0\n",
    "val_loss_benchmark=1000\n",
    "for i in range(10000):\n",
    "    train_input, train_target, val_input, val_target=corpus.get_minibatch()\n",
    "    train_input=Variable(train_input.cuda()).permute(1,0)\n",
    "    train_target=Variable(train_target.cuda()).permute(1,0)\n",
    "    val_input=val_input.cuda().permute(1,0)\n",
    "    val_target=val_target.cuda().permute(1,0)\n",
    "    train_loss=train_minibatch(train_input, train_target, hidden_size, mini_batch_size, model, model_optimizer, criterion)\n",
    "    val_loss=validate(val_input, val_target, hidden_size, val_size,criterion)\n",
    "    batch_training_loss+=train_loss\n",
    "    batch_validation_loss+=val_loss\n",
    "    if (i+1)%100==0:\n",
    "        tl.append(batch_training_loss)\n",
    "        vl.append(batch_validation_loss)\n",
    "        print ('Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format((i+1)/100, 100, batch_training_loss, batch_validation_loss))\n",
    "        if (batch_validation_loss<=val_loss_benchmark):\n",
    "            print ('%---Saving the model---%')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_optimizer_state_dict': model_optimizer.state_dict(),\n",
    "                },'models/LangModel.pth')\n",
    "            val_loss_benchmark=batch_validation_loss\n",
    "        batch_training_loss=0\n",
    "        batch_validation_loss=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangModel(\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (emb_layer): Embedding(14839, 300)\n",
       "  (rnn): GRU(300, 512, bidirectional=True)\n",
       "  (Linear): Linear(in_features=1024, out_features=14839, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('models/LangModel.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_optimizer.load_state_dict(checkpoint['model_optimizer_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The END. \"\n",
    "temp_begin = re.findall(r\"\\w+|[^\\w\\s]\", sentence[:].lower(), re.UNICODE)\n",
    "temp=torch.from_numpy(np.array([corpus.subwords_stoi[k] for k in temp_begin],np.int64))\n",
    "temp=temp.unsqueeze(1)\n",
    "temp=temp.type(torch.cuda.LongTensor)\n",
    "temp.size()\n",
    "newOutput=[0]\n",
    "for i in range(50):\n",
    "    with torch.no_grad():\n",
    "        hidden_state=torch.zeros(2, 1, hidden_size, device=device)\n",
    "        outputs = model(temp, hidden_state)\n",
    "        newOutput = F.softmax(outputs,dim=1)\n",
    "        #print(newOutput.size())\n",
    "        (newOutput,indices)=torch.topk(newOutput[-1],8)\n",
    "        #print(newOutput,indices)\n",
    "        #newOutput=newOutput.max(1)[1][-1]\n",
    "        newOutput =torch.multinomial(newOutput,1)[0]\n",
    "        #print(newOutput)\n",
    "        newOutput=indices[int(newOutput)]\n",
    "        newOutput=newOutput.view(1)\n",
    "        #print(newOutput)\n",
    "        \n",
    "        temp=torch.cat((temp, newOutput.unsqueeze(1)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'end',\n",
       " '.',\n",
       " '\\\\',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'why',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'want',\n",
       " 'any',\n",
       " 'crap',\n",
       " '-',\n",
       " '-',\n",
       " 'go',\n",
       " 'quick',\n",
       " 'step',\n",
       " '-',\n",
       " '-',\n",
       " '\"',\n",
       " '\\\\',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'why',\n",
       " 'are',\n",
       " 'you',\n",
       " 'okay',\n",
       " '-',\n",
       " 'a',\n",
       " 'bun',\n",
       " 'bun',\n",
       " 'thing',\n",
       " 'from',\n",
       " '\"',\n",
       " 'yes',\n",
       " '?',\n",
       " '\"',\n",
       " '\\\\',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'yes',\n",
       " '?',\n",
       " '\"',\n",
       " 'said',\n",
       " 'harry',\n",
       " 'curiously',\n",
       " 'curiously',\n",
       " 'curiously',\n",
       " 'curiously',\n",
       " 'curiously',\n",
       " 'curiously',\n",
       " 'curiously']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[corpus.subwords_itos[int(k)] for k in temp.detach().cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in range(1000):   \n",
    "    temp_begin = corpus.sp.EncodeAsPieces(\"Daha once bu yonet\")\n",
    "    temp=torch.from_numpy(np.array([corpus.subwords_stoi[k] for k in temp_begin],np.int64))\n",
    "    temp=temp.unsqueeze(1)\n",
    "    temp=temp.type(torch.cuda.LongTensor)\n",
    "    temp.size()\n",
    "    newOutput=[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden_state=torch.zeros(2, 1, hidden_size, device=device)\n",
    "        outputs = model(temp, hidden_state)\n",
    "    a.append(corpus.subwords_itos[int(outputs.max(1)[1][-1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
